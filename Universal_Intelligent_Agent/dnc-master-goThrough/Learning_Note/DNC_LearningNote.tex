\documentclass{article}
\title{Differentiable Neural Computer Learning Note}
\author{Zhengzhong Liang}
\date{2018-04-30}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\begin{document}
\maketitle
\section{Code Structure}
\subsection{About Computation Graph}
The basic graph is defined in ``DNC.py'', some peripheral parts of graph is defined in ``run$\_$model''function and ``train.py''. The main parts of computation graph of DNC is organized as following:
\begin{table}
\centering
\caption{Organization of DNC}
\begin{tabular}{| l | c | c | c | r |} \hline
name & functionality & script & function & class \\ \hline
controller & an LSTM network which reads data & DNC.py & x & DNC \\ \hline
access & the memory of DNC & access.py, 
\end{tabular}
\end{table}

\subsection{Activities in Each Loop}
In each loop of training in iteration, the agent does the following things: 
\paragraph{Some input is presented}
\paragraph{Input is given to LSTM}
\paragraph{Something is written to memory}
\paragraph{Something is output from memory}
\paragraph{The output is decoded by LSTM and printed}


\section{Topics to Study}
\subsection{What type of task does this agent try to finish}
\subsection{What is the function of memory in this task}
\subsection{What together tasks can we train the agent to do}
\subsection{What parts and functions can we add to this agent}

\section{Question}
\subsection{The memory and controller are defined, but are not connected}
They are connected in the build function, which is called by dynamic rnn function.
\subsection{The build function is not used}
They are used in dynamic rnn function.
\subsection{The functionality of dynamic rnn}
It connects the different parts of the computation graph of DNC, so that a complete graph will be established after calling this function.
\subsection{In every loop a new DNC is created?}
No, the DNC is only built once. 
\subsection{How the computation graph is built in tensorflow?}
Anything before a session starts is considered as computation graph.
\subsection{What does build do in the code}
Connects different parts of computations graph. Build function should be orginally a method of snt.AbstractModule. The abstract module is define 
\href{https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py}{here}.

As one can see, the build function is a method in this class.
\subsection{Draw UML for the code}
See the UML diagram in the folder.
\subsection{Why there is a init function and a build function in each class}
init function creates the components of the computation graph, while the build function connects these components. The init function is called when the object of DNC is initiated. The build function is called in the tf.nn.dynamic$\_$rnn function.
\subsection{The sequence of calling of build function}
DNC, Access, Freeness, ConsineWeights, TemporalLinkage, ConsineWeights.
\subsection{Draw the inheritance relation ship (and attributes) starting from AbstractModule}
\subsection{Figure out how abstract class works in python (e.g. why no init function)}

\section{Questions about DNC}
\subsection{Equivalence between Neural network, Turing machine and brain?}
It is believed that RNN is Turing Complete (Turing complete seems mean that something is equivalent to Turing machine.). And it is believed by some people that brain is not more powerful than Turing machine. So that we can conclude that brain$\neq$RNN$=$Turing Machine.

However, it is questionable then that why we need external memory attacked to RNN if RNN alone is Turing complete.
\end{document}